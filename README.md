# Capstone Project
Capstone project created to fufill degree requirements

Over the past semester, I have worked independently on a programming project. This was a virtual machine that would run machine code, and programs that would translate higher level code to machine code to be run in the virtual machine. The purpose of this project was to serve as a compilation of everything I have learned in my academic studies and explore a certain area of computer science.

The original inspiration for my capstone project came from the main assignment in my Logic Design course, where we utilized the textbook The Elements of Computing Systems: Building a Modern Computer from First Principles. The textbook was an explanation of computer system design from the hardware level to high-level programming, along with how each layer supports the next. Following the text, we programmed a virtual computer machine using outlined designs and algorithms. I became fascinated with the idea of simulating a computer interpreting machine code and how the instruction sets are designed. Something that we did not spend a lot of time on in class were the higher-level applications of the assembly code. I also wanted to expand the scope to include a stack language that would allow for functions and further high level concepts to be performed using the virtual machine. 

Outside of my courses, I have always had an interest in video game emulator programs. These are programs meant to recreate the hardware of a video game console to run game data accurately to the point of there being little to no difference. The research and documentation on the original hardware has to be extensive in order to properly run the game program. These implementations tend to go no deeper than the CPU level, since the exact operation of logic gates does not need to be simulated in order to be accurate. It was this area of the hardware that I felt the most interest in because of the unique decisions on CPU instruction set creation. A balance has to be kept that will limit the available functionality in ways that turn programming into a puzzle of maximizing efficiency.

My original plan for the project was to create programs that will do translations from a stack language to assembly, assembly to machine code, and then execute that bytecode. I also planned to cut out the scope of designing the CPU, RAM, and other chips for the machine with logic gates and chip design software as that felt extraneous to my main focus of programming. If this project ended up being completed way earlier than intended, I would try to create an even higher level language translator that would turn a more common syntax of code all the way to machine bytecode through the processes.

The main alteration I made in order to make the designs different from the ones used in the Logic Design class was to change the CPU instruction size from 16 bits to 8 bits. This was done in consideration to the development of embedded systems, which are computers placed within other objects such as cars or household appliances. These systems have to be much more limited in computing power because of cost. I found this limitation in hardware to be a very interesting problem to work with and wanted to explore that in my project.

Initially, I adapted the “hack” machine and assembly language as a starting point. This is the language outlined in the textbook and utilized in my previous class as a starting point. I was already familiar with the syntax and the instruction set layout. I found it more accessible than most other assembly languages. Having to adapt the language to work within an 8-bit instruction set turned out to be the cause of many further challenges to work through.

The first obstacle encountered in designing for the 8-bit cpu was having my operations very limited by adapting the designs without considering the new limitation. The hack assembly language is made up of A and C statements. The A statements count as 2-byte values and are loaded into the A register when executed, while the C statements perform both the arithmetic and control flow of the program. C statements can also be placed in any of the registers, or used as the value to test if a jump occurs if a condition is true. The main signifier between the A and C statements was the single leftmost bit. This would require that values loaded into memory were limited to 15 bit integers.

When adapting the hack assembly language for my own design, I still used the leftmost bit to differentiate between A and C statements, meaning that I was restricted to 7 bits for my values loaded into memory. This also had the effect of restricting my accessible memory in the A register to 127 possible addresses. This would mean that the machine would have only 127 bytes of memory accessible. Alongside this limited memory access, trying to squeeze math, the destination for the result, and whether or not a jump should happen into one 7-bit string was overly constricting. While it proved to be an interesting challenge to work with this limitation for the first few weeks, it proved to be too much and I decided instead to look for a solution in existing 8-bit cpu design.

I decided to research what Instruction Set Architectures were used for 8-bit CPU’s and I came across the Intel 8008 ISA. I found that it made use of an alternative to A statements called “Immediate Loads”. This is an instruction that tells the CPU to treat the next byte as a value, and then do an operation with it. This allows registers and memory addresses to store the full 8 bytes, and also doubles the possible instruction set size from my first iteration.

With this change in mind, I was able to plan out a better machine for the project. I expanded the instruction ROM and memory space to 16 bit addressing, allowing 64k memory bytes and 64k instructions to be run in a ROM space. This required that the memory access pointer be made up of two bytes, the high address “H” and the low address “L”. To facilitate 16 bit values better, two general purpose registers “A” and “B” were also implemented, replacing the single D register. The A register also determines the value tested in jump commands and acts slightly as an accumulator.

With the design settled I was able to create my own virtual machine for the system I planned, and was able to write assembly code for that system. For programming I used the Java language out of its built-in library features that used to create a UI for debugging along with several other built-in data structures like Lists and Hashmaps. My main concern with using Java for this project were performance issues, as the code interpreting my machine code as a virtual machine would all have to be interpreted by the JVM. If the virtual machine was written in a compiled language like C++ or FORTRAN it would be much more feasible to run this interpretation of the machine code closer to real time.

A vital feature in the hack assembly language is the usage of symbols. These are labels handled by the assembly compiler to point to specific memory or instruction addresses. They are an intermediary step and do not show up in the final compiled machine code. The purpose of these is to allow for program flow control by setting the access registers to specific memory addresses Since the rework changed addressable memory to take two bytes to access in the H and L registers, loading those values into memory would take 3 bytes. I also set up the translation to instead use the instruction to only load a value into the L register if the value is under 255, because that would instead take up two bytes of instruction. This kind of optimization was unnecessary in hindsight because of the issues it would cause later.
Translating the hack assembly language to binary machine code the process for handling labels was a two step process. First, a table of labels will be created containing the name, and where they should point to in the code. Then, when statements are converted to binary any instance of a label being loaded into H and L with an “@” statement the table is consulted to see what position of the label should be loaded in. If it is less than 255, the 2 byte method is used and otherwise the 3 byte method. An issue appeared when trying to figure out where in the program the label is in the code. Jumping to a label in instruction memory would be off and completely break functionality in code that went past 255 addressable bytes. My first attempt involved creating a function that would return how many bytes an instruction would take up, and using that to try and estimate the proper location of a label, but this proved trickier because of the @ statements taking a varying amount of bytes based on what is being accessed. This eventually led to another pass needing to be made over the program code to store a list of each instruction’s location in terms of statements, and using that to more accurately determine where it should properly point to.

Once these issues with the assembly translator were resolved, I could then begin work on another layer of the project. This layer of translation is commonly referred to as the VM language because of it being a translatable intermediary to any CPU instruction set. In order to avoid confusion with the actual virtual machine, I referred to it as the stack language in my documentation. The purpose of this language was to create a stack environment to run programs in, to allow for the procedural function model to be implemented, where functions can be calle in other functions, and returning will bring the program back to where the function was called. Since statements performing stack arithmetic are the same every time they are executed, I found that creating a “library” of assembly code for use by the translator was the most effective. When a statement was in the list of simple stack arithmetics, a file containing the code to perform that arithmetic was added to the output translation.

For more complex statements, more work had to be done. Some common statements in the stack language are “pop” and “push”, these statements will move around bytes of data off and onto the stack data structure. When implementing these statements, I also followed the design present in the textbook. After the statement a memory page and an address offset would be defined to pick out which byte of memory was needed at what address. Since these arguments for the statements can vary, I decided to create a preprocessor language for my assembly code. This allowed for specific keywords to be swapped out with correct statements, along with simple preprocessor if and else program control. This proved useful for also implementing capability for functions.

There was one emergent issue from using a library of assembly code and templates to stitch together a final program. Some of these portions of assembly would make use of the label and jump system described above. They would be placed in the same program multiple times to execute stack functionality. To solve this, I marked certain labels to be replaced by the stack machine translator to be made unique by adding a number and the function currently being translated. This stops the label table from getting confused with multiples of the same label.

If I had the opportunity to work further on this project, I would add another layer of a high level interpreted language to give the stack language a better use. I was investigating how to properly implement the C language as a “what-if”, but what I found instead was that the exact definitions and compilers for C are much more complex than I initially anticipated. I reasoned instead that if I were to support a high level language it would have to be a very simplified version of an existing language, with the intricacies removed. Something else that I had plans for was some kind of graphical display for the virtual machine, using a set of specified bytes to represent graphical data in a window. This did get implemented part way but I prioritized working on the stack language over this. I found implementing the language and its translation much more interesting than working with graphical display.

While I did use Java for what I believed to be easier use, a functionality I didn’t make enough use of was its Object Oriented Programming structures. A majority of my algorithmic design for the programming portion was built very procedurally oriented. A whole refactor to better make use of OOP structures would allow my program to make better use of the Java language.

One last thing I would re-evaluate if I had more time would be my design of the virtual CPU and assembly language. When I expanded the instruction set to any possible 8 bits, there were an excess of commands that would either do the same thing as another, or nothing. After programming in the assembly for the CPU, I believe there are a number of new functionalities that should be there for ease of use. A more proper way to go about this would be to model the virtual CPU itself to make sure that all the instructions are possible to perform with circuit logic. This would be going down to intricacies that I initially cut out of my scope, but it would be a required step in order to keep some kind of authenticity to my simulated virtual machine.

I am very satisfied with what I have managed to finish within the semester. Since the programming algorithms for translating the code from a stack language, to assembly, and then to machine code were things I already did in an earlier class, I used the opportunity this semester to go into it with a unique specification that brought its own challenges and perspective on creating this kind of simulation.
